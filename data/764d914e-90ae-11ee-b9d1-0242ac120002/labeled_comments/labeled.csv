comment_authors,post_info,comments,seen,toxicity,all
TheHelpfulAssistant,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"I found the following article interesting.

[A Deep Dive Into Guidance’s Source Code](https://betterprogramming.pub/a-deep-dive-into-guidances-source-code-16681a76fb20)",False,False,"[0.006714871, 0.01173141, 0.0027378413, 0.0073426575, 0.00816794, 0.2681796, 0.006686311, 0.00081062317]"
scratchr,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"Guidance is a DSL (special domain specific language, kind of like handlebars or SQL) for constructing and prompting LLMs. The LLM doesn't understand the guidance language. The guidance library fills in your variables using the template syntax and then runs generation during the appropriate template elements. It only runs generation for that part and then switches back to prompting, which allows it to enforce data structure much more effectively.",True,False,"[0.0068028234, 0.01017851, 0.0074368757, 0.012517073, 0.007047772, 0.30984423, 0.0029228306, 0.0007677078]"
Meeterpoint,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,When comparing alternatives for a project I perceived as guidance being a bit stale recently. I personally went for lmql which is in a way similar but also very actively developed.,False,False,"[0.00414376, 0.017718147, 0.24106604, 0.014498309, 0.008787598, 0.0107048955, 0.0009250641, 0.007249452]"
Robot_Graffiti,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"The {{}} syntax interpretation is mostly done by a normal computer program, not an LLM. That's what makes it fast (LLMs are slow). It's converted into a series of prompts, which are fed into the LLM.

The gen and select commands cause the LLM to generate text. Other commands insert text into the prompt with faster non-AI methods. Everything that's not a {{}} is just part of a prompt.

LLM models don't choose the next token on their own - the model outputs a list of token probabilities, and then a program uses some algorithm to choose a high probability token. Guidance's select command controls this step with a list of allowed strings. The pattern option in the gen command controls it by not allowing tokens that don't fit your regex pattern.

    guidance(""My momma always said {{gen 'saying'}}. My momma was {{#select 'what_she_was'}}a saint{{or}}an idiot{{/select}}."")

should send this prompt to the LLM

    My momma always said 

and then it'll start generating text until it outputs a . Let's say it generates ""life is like a box of chocolates""

Then the LLM will run with this prompt:

    My momma always said life is like a box of chocolates. My momma was 

And then the LLM will either generate ""a saint"" or ""an idiot"".",False,False,"[0.008220384, 0.03592727, 0.012739069, 0.32810524, 0.0076215584, 0.022286616, 0.017356457, 0.0018501282]"
sergeant113,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"So, it’s like a prompt generation engine. Do we know how token-heavy it is compared to direct prompting? How does it handle a long conversation where natural language messages are mixed among task-command messages?",False,False,"[0.26194692, 0.008614883, 0.014450971, 0.0007915497, 0.0067769317, 0.012175481, 0.006605443, 0.0026823445]"
Disastrous_Elk_6375,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"Same. I thought that being under the MS umbrella would insure funding, dev time and all that good stuff, but sadly it hasn't been the case. Let's hope it's just the summer time vacation and they'll be back.

I've also switched to lmql, can't wait for them to finish the caching stuff. Compared to guidance lmql is still very very slow. Caching should improve it a bunch.",False,False,"[0.0027563404, 0.011198899, 0.007094103, 0.00089645386, 0.23950863, 0.013336895, 0.005042548, 0.01822079]"
deepinterstate,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"I would love to see an example python script that managed to do this using the oobabooga API or the koboldAI api or something.

I tried to get guidance working with a local llm and failed rather spectacularly, which was annoying because I had no trouble using the API for all sorts of other things. I gave up on it and decided to come back when someone had a better example out there, but so far, I haven't found one. I don't want to use guidance with openAI - just a local model.",False,False,"[0.02308189, 0.0011825562, 0.0071847234, 0.0074311234, 0.36012006, 0.019280603, 0.012148905, 0.0030523231]"
teleprint-me,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,Then why use an LLM at all at this point? Doesn't this defeat the purpose?,False,False,"[0.012300906, 0.0032373124, 0.0010061264, 0.007145886, 0.013849284, 0.27822426, 0.0060746484, 0.027324399]"
kc858,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,+1 please let me know if you find one,False,False,"[0.31199628, 0.00819688, 0.017162729, 0.0043657473, 0.016210219, 0.014862247, 0.008841781, 0.0014877319]"
Disastrous_Elk_6375,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"It makes a new request for each generate instruction. The advantage of using guidance with local models is that they also do some caching along the way, so the work isn't re-done on every query. It kinda works from where it left off. 

The problem is that the repo seems to be abandoned for the moment. There's been about a month break with no commits, and then one singular commit about 2 weeks ago. There are some bugs, and lots of issues & PRs on github. The discussion about plans and future support haven't been answered by the maintainer.",False,False,"[0.016556932, 0.013092931, 0.00933372, 0.03309893, 0.002040863, 0.007547563, 0.23658851, 0.02385794]"
Robot_Graffiti,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"I gave a goofy example to keep it short. But no, the purpose is not defeated. The purpose of Guidance is to make it easier to integrate AI into a computer program that processes data, it's not needed for freeform roleplay waifu chat. It's still using the LLM to understand human language and culture, it just allows you to guarantee the answer to be in a specific format.

Guidance has three benefits. For convenience, you can write a complex sequence of prompts as one query. For speed, it is a more efficient way of getting machine-readable data out of an LLM than asking it to write in JSON or any other specific format. For reliability, it guarantees that the answer you get isn't unparseable broken JSON or ""As an AI language model, I can't read suddenly, I don't know.""",False,False,"[0.23937434, 0.00289917, 0.0050317086, 0.058475737, 0.0069905366, 0.013446795, 0.11140333, 0.068967216]"
kryptkpr,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,Maybe it's a conspiracy theory but I wonder if they stopped developing or just stopped sharing their work on GitHub or maybe even got in trouble for the local support.  Microsoft is a weird sort of frenemy of local in this space..,False,False,"[0.008686432, 0.04207976, 0.030389199, 0.010969753, 0.33002734, 0.022188433, 0.112953395, 0.0023937225]"
teleprint-me,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"The example isn't the issue. You can do this without an LLM. For more control, you can use something like Context Free Grammar which already has powerful implications on its own.

Using the proper template structure as a guide can improve the output, but for rigorous structure and fixed templating, the use of an LLM is complete overkill (a waste of compute and resources).

The only context this makes sense within is an Agent script, e.g. a Help Desk Assistant, and even then, it restricts the proactive elements by coercing its outputs.

""Guidance"" can mean anything, but to adhere to the context at hand, it's a Structured Query Language used to coerce a specific sequence of outputs based on directed inputs, e.g. Prompt Templates. You can do this without a programming language and unfortunately, it will never be model agnostic. No matter how much we would like it to be. DSL's have their place, but this isn't it.

To enforce my disposition and argument even further, we can explicitly reference LMQL as a specific example.

```
sample(temperature=1.2)
    ""Say 'this is a test'[RESPONSE]"" where len(TOKENS(RESPONSE)) < 25

    if ""test"" not in RESPONSE:
        ""You did not say 'test', try again:[RESPONSE]"" where len(TOKENS(RESPONSE)) < 25
    else:
        ""Good job""
from
    ""openai/text-ada-001""
```

This is overkill. I can program this in a more efficient way. I might as well not even use an LLM here. Yes, I recognize the triteness behind the examples and understand how they can be utilized, but it's still a waste of time.",False,False,"[0.041172907, 0.019934712, 0.017709278, 0.008789998, 0.27243996, 0.0015163422, 0.006516977, 0.0028673338]"
Disastrous_Elk_6375,How does Microsoft Guidance work? on r/LocalLLaMA by T_hank,"My hope is that it was just the summer vacation availability and they'll be back to supporting the project, as it was by far the best when working with local models.

LMQL is also really promising, and is maintained, they have a great roadmap of features, and are working on caching so that's gonna make it much faster than it is right now.",False,False,"[0.0023493639, 0.005160502, 0.007822373, 0.011560614, 0.007151874, 0.0007390976, 0.0068287146, 0.21148337]"
seanstar555,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"From my own experience tinkering around with both, LangChain is nice when you're just starting out, but if you're willing to put in the effort manually implementing things, guidance will give you more advanced control over the project. LangChain includes a lot of functionality out of the box, and it's an easy to use all-in-one solution for most of the things people want to make. However, there are a lot of parts of LangChain that are designed to work with other parts of LangChain, making it difficult to use your own implementations for certain parts of your project that you're working on (Whether it's something your project requires, or you just think you could do it better). Guidance kinda forces you to create your own solutions to certain things, but as a result you have more control over your implementation, and it's more efficient. For your projects, it may be most suitable to use either one or the other depending on your requirements. If LangChain alone can do everything you need it to do, I would suggest going that route to save the time, but if you want to take advantage of guidance's efficiency and more advanced prompting, you'll have to be more technically involved.",True,False,"[0.0035702933, 0.0011777878, 0.007832011, 0.010666896, 0.017299367, 0.010438959, 0.018723432, 0.21304017]"
kryptkpr,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"LangChain has excellent documentation with many examples and lots of projects that use it.

Guidance hurt my brain. I don't understand their abstractions, I don't understand how to use any of it, I stared at their demo apps for 2 hours like an idiot and couldn't get a single useful modification to work.

Summary is I am too stupid for Guidance, so I use LangChain.",False,False,"[0.47886392, 0.5177041, 0.34099978, 0.027129497, 0.24648386, 0.013345404, 0.007586042, 0.013995127]"
synkr3tyk,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"On what planet are Semantic Kernel and Guidance closed source? Both are released under the MIT License, as noted plainly on the GitHub project homepages for each.

I prefer Guidance to LangChain. LangChain has a slightly richer set of features, but as a software project it's a mess. In the end I'm much more productive with guidance - it's easier to use, the examples and source code are clearer and better-organized, and they seem to have a plan with respect to a roadmap.",True,False,"[0.021868404, 0.0042732526, 0.0074901003, 0.33086276, 0.0021839142, 0.0063432488, 0.06381883, 0.06195303]"
extopico,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,Is it not possible to use guidance with langchain? I have not tried because I am still waiting for llama.cpp support inside guidance.,True,False,"[0.008823884, 0.009613278, 0.004088263, 0.0138151245, 0.0076119336, 0.2311539, 0.015833238, 0.0010681152]"
Dangerous_Aardvark15,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"Am I allowed to use Guidance along with my own llm, one that I import from hugging face? I can't seem to generate a response with guidance.

I am using:

\`tokenizer = AutoTokenizer.from\_pretrained(""lmsys/fastchat-t5-3b-v1.0"")model = AutoModelForSeq2SeqLM.from\_pretrained(""lmsys/fastchat-t5-3b-v1.0"")\`

\`guidance.llm = guidance.llms.Transformers(model = model, tokenizer=tokenizer)  
prompt = guidance('''  
{{model\_instruction}}  
{{#each top\_k\_chunks}}   
 {{this.content}}  
{{/each}}  
{{query}}  
{{gen 'response'}}  
''')

answer = prompt(model\_instruction = loose\_instruction, top\_k\_chunks=top\_k\_chunks\_dicts, query=query)\`  


The code runs but no response is generated. I am not sure why. :<",False,False,"[0.011767523, 0.035399333, 0.00984143, 0.00333786, 0.042566355, 0.3710805, 0.041915078, 0.014390918]"
Old-Upstairs-2266,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,Semantic Kernel is the best: [https://www.youtube.com/watch?v=90hhJHTWz50](https://www.youtube.com/watch?v=90hhJHTWz50),False,False,"[0.014513801, 0.017572641, 0.007645877, 0.0014686584, 0.5423651, 0.007534259, 0.0049577127, 0.015452018]"
Caesar_Zalad,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,">hing you need it to do, I would suggest going that route to save the time, but if you want 

Thanks for the input! Have you had any experience with Semantic Kernel as well? Seeing as both Guidance and Semantic Kernel are Microsoft related, my initial thought was that it might be more sustainable and well-researched going forward on a longer-term project. Though both seem quite static and I quite like the speed at which LangChain is operating.",False,False,"[0.007987879, 0.0040512653, 0.0077849864, 0.001206398, 0.37484214, 0.007819066, 0.014805742, 0.012126249]"
nbuster,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"You're not wrong! I thought it was me, but hey, your post gave me hope.",False,False,"[0.0024418586, 0.00086307526, 0.019854378, 0.0067233974, 0.010894896, 0.006699257, 0.0128245065, 0.24010436]"
nbuster,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"Both guidance and langchain expect an adapter to be instantiated for your LLM today.
They are not, to my knowledge, able to coexist and force you into having two LLM instances. 
This could probably be mitigated, albeit not easily as you would need to abstract some of the LLM's inner-workings (logprobs) into an API, then build an adapter on top of it.",False,False,"[0.009173225, 0.39003542, 0.006770459, 0.002034882, 0.011048226, 0.0056323195, 0.0006532669, 0.007170874]"
Smallpaul,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"There's no guarantee or reason to believe that they would be more sustainable and well-researched going forward. Big companies kill projects all of the time.

Also: are Semantic Kernel and Guidance designed to be used together? My quick skim didn't provide any evidence that they are. If they are two unrelated projects then there's no reason to think of them as a unit.",True,False,"[0.012926984, 0.42567968, 0.038048524, 0.013934915, 0.0012683868, 0.0027424383, 0.0025713511, 0.011974656]"
seanstar555,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"I don't have experience with Semantic Kernel, but from a brief look at the documentation they have for it, it looks like it's supposed to be an  all-in-one sdk for for creating agents and other apps. Overall, seems like another more powerful approach at the cost of usability from Microsoft. It's hard to say what the landscape is going to look like too far into the future, so I'm not really going to attempt to make any guesses.",False,False,"[0.016838523, 0.0012111664, 0.3406098, 0.0056236745, 0.008323951, 0.009241886, 0.011264639, 0.016616182]"
kryptkpr,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"I understand how guidance works in theory: it's doing max_tokens=1 and playing with logit propabilities so the output can always be constrained.  This is a great idea!  But they've created a highly opinionated implementation based on a custom markup language that they ""execute"" in entirely unclear ways.

If guidance code could be written via plain python APIs it could be 10x more useful imo",False,False,"[0.0046002194, 0.006543908, 0.011937759, 0.0086718835, 0.0007343292, 0.42310786, 0.0018498929, 0.011816809]"
extopico,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"Ok, got it. I was thinking along the lines of writing an agent that is an LLM governed by guidance, and this agent is in turn invoked by langchain to perform a task.",False,False,"[0.006770459, 0.007106749, 0.000834465, 0.012961143, 0.33174738, 0.017592486, 0.010096892, 0.003385304]"
seanstar555,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"The guidance transformers wrapper works with a loaded huggingface transformer model/tokenizer, so if you loaded the model with huggingface transformers and you made a custom LLM wrapper for langchain that uses huggingface transformer model/tokenizer input, you may be able to load the model once and then create/use both llm objects. Though I'm not really sure why you may want to do that.",False,False,"[0.0013160706, 0.015334064, 0.0084818825, 0.017843807, 0.2995488, 0.0043102503, 0.016752819, 0.008155655]"
seanstar555,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"From what I could gather giving it a brief look, they're two completely different things that just happen to share similar markup for the prompting.",False,False,"[0.026617315, 0.017026093, 0.01309891, 0.0042177555, 0.21110031, 0.007301235, 0.012385205, 0.0010967255]"
classicboyir,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"Although they are two different projects, here is how to integrate the two: https://github.com/microsoft/semantic-kernel/pull/1550",False,False,"[0.0094363475, 0.0009393692, 0.007009373, 0.18808645, 0.009173225, 0.0034962974, 0.0073789097, 0.013131939]"
Caesar_Zalad,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"u/Smallpaul, you're right that a sustainable project is never a certainty, certainly with big tech's history. Yet I don't see Microsoft abandoning LLM controlling projects anytime soon and together with their OpenAI partnership, even ending up being quite potent. So far, there are no options to integrate them, but they do seem aware. ([https://github.com/microsoft/guidance/discussions/200](https://github.com/microsoft/guidance/discussions/200))

&#x200B;

u/seanstar555 Semantic is indeed a full on sdk. They never made a comparison with LangChain because that wasn't their focus at the moment (according to: [https://github.com/microsoft/semantic-kernel/discussions/1326](https://github.com/microsoft/semantic-kernel/discussions/1326)). Yet I was hoping that someone with experience in both might have a cent or two to say as a form of comparison. 

Its indeed tricky to peek even slightly into the future, but I was seeking other people's experiences to make up my own mind as to how it might look. 

Currently tipping toward LangChain because of the speed at which I can get my implementation done. I reckon I'll just have to spend some extra effort and time in the future on custom components.",False,False,"[0.016556932, 0.0021076202, 0.035691574, 0.008686432, 0.2888407, 0.009619443, 0.01792906, 0.026863953]"
extopico,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"I cannot use transformers directly due to insufficient hardware. As to why and how, as I said I have not been able to try either due to the hopefully soon to be fixed missing implementation of llama.cpp and reading about the architecture is different to attempting to use it, at least to me as I’m rather hands on. 

In essence I’m not planning on holding multiple instances of an LLM alive, but sequentially. Llama.cpp for example allows model persistence so even when the task is done and a python program (agent?) is finished, the model is not unloaded and can be reused by the next program.",False,False,"[0.0028118372, 0.016342908, 0.02035702, 0.007401635, 0.33544827, 0.0011634827, 0.010932897, 0.007365964]"
seanstar555,The best framework currently going forward: LangChain vs Microsoft (Guidance and Semantic kernel)? on r/LocalLLaMA by Caesar_Zalad,"If you haven't already, I would go over the Guidance's examples on their readme and make sure that you're not going to require that sort of functionality in the future. It would be much easier to implement LangChain's functionality using Guidance than it would be to implement Guidance's functionality using LangChain with what we have currently. Specifically look at the examples that keep the model on the rails (or format) in it's response, as that's clearly what they intended to bring to the table with Guidance and probably the single most useful feature Guidance offers when it comes to locally hosted, low parameter models. If that's something that you know you won't require for what you're trying to do, then going with LangChain would be the obvious answer, at least currently in my opinion, just due to the popularity and speed of development it's had by comparison. I wouldn't be surprised if LangChain implemented similar functionality to guidance in the future, either, considering how useful that sort of thing is for instruction based applications using small locally hosted models. Though it's not the current focus, LangChain definitely has broader goals in mind.",False,False,"[0.012680092, 0.22242196, 0.0014400482, 0.008453408, 0.013668913, 0.0050687063, 0.019280603, 0.026028076]"
Disastrous_Elk_6375,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,You want to use the guidance library from MS. It's great for this.,True,False,"[0.0074901003, 0.012961143, 0.013571346, 0.008633883, 0.2863294, 0.0071717775, 0.002830336, 0.00094890594]"
LionaltheGreat,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I was JUST doing this for a work project. I really like the suggestion of the MS guidance library

What I did, is define 2-3 manual examples of what I’m wanting. Then I had GPT4 create more 20 examples following my original JSON output example. 

Then embedded that in my prompt, before giving the task at hand",False,False,"[0.010497936, 0.0077025536, 0.0011968613, 0.0037922803, 0.015488927, 0.2937965, 0.007968879, 0.01237757]"
TKN,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"In addition to giving it some examples first (and testing with different temperature etc. parameters that others have mentioned) maybe forcing it in the right direction by inserting the beginning of the JSON object as it's response would work? As in:

User: Give response as a JSON object with properties ""foo"" etc

AI: {""foo"":

Then just cut the answer at "","" add the name of the next property (if not inside a quoted string) and so on.

Doing it in multiple passes might also be worth trying, first generate the response and then ask it to format it to JSON. That way you could use different parameters for generating the actual response content and it might help smaller models in following the instructions.",False,False,"[0.019934284, 0.010084574, 0.013896915, 0.028974265, 0.2355923, 0.0023269653, 0.026317406, 0.004587734]"
TechnoByte_,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,You should put some example answers in the prompt,False,False,"[0.014703264, 0.007819066, 0.25659752, 0.01608456, 0.0011348724, 0.0037182847, 0.010674868, 0.008500882]"
aalyushin,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I was experimenting with different prompts and LLMs for the case scenario to extract information from articles, and Guanaco-65B was performing well in 99% of the cases with prompts similar to this one (partly taken from Langchain documentation):  


Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  
\### Instruction:   
Extract the following information from the text in input:  
1. Article ID  
2. Publication date  
3. Article title  
You must format your output as a JSON value that adheres to a given ""JSON Schema"" instance.  
""JSON Schema"" is a declarative language that allows you to annotate and validate JSON documents.  
Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!  
As an example, this text: {sample text here}  
Results in the json: {provide your desired output structure}  
\### Input:   
 {article text}  
\### Response:  
Valid JSON document that adheres to the schema output schema instance {your desired schema} : \`\`\`json {",False,False,"[0.022833163, 0.013174911, 0.008657498, 0.0017166138, 0.027088705, 0.021821553, 0.27556, 0.0103175985]"
neph1010,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I'm working on the same thing, right now, and I've had great success using one example of the format you want and using mirostat sampling. It not only improves the quality of the response (so far), but also makes the replies consistent, which is key when experimenting with prompts. Also telling it in the instructions to 'only output valid JSON' helps. I'm using chronos-hermes-13b-ggml, so not even a fancy 'coder' model or an especially large one.",False,False,"[0.0011396408, 0.37771347, 0.0092228865, 0.004162259, 0.01608456, 0.007935578, 0.011323617, 0.017094411]"
MaterBumanator,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"We have been working on a similar problem, with the exception we are fine-tuning our own models, so we have some control of the model output.  In our case there was better performance outputting the discrete outputs we wanted and parsing them to json than training to output JSON directly.  In this case jsonformer and MS Guidance were more error prone than a simple (coded) output.  This being said, we have used MS Guidance successfully elsewhere.  

I have not been able to find any information on how to best generate training sets for multi-class/multi-label extraction to a single JSON.  We are currently experimenting with various input and output structures.",False,False,"[0.014942379, 0.013554913, 0.02343543, 0.0072365063, 0.0030708222, 0.0010204315, 0.006399023, 0.24553548]"
sgt_banana1,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Give it a name and a prompt along the lines of ""as *name*, you only provide answers in JSON format


Then hello *name*, please do the following:
1.
2.
Provide your answers in JSON with the following keys:
Foo,bar",False,False,"[0.32201704, 0.0019168854, 0.0038107792, 0.007819066, 0.019854378, 0.018872695, 0.022218296, 0.009678889]"
Opening-Ad1642,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,You can program a standardized and constrained output using LMQL https://lmql.ai/,False,False,"[0.0009059906, 0.0070663732, 0.3278728, 0.0029413297, 0.010497936, 0.009613037, 0.013405213, 0.007145886]"
Jakekill871,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I’d highly recommend either jsonformer or prompt engineering with StarChat Beta, XGen 7b, and Raven v4 14b (World, the newest version isn’t as good at output parsing) for all of these I recommend no repetition penalty, multi shot, around 0.6 temp and 0.7 top p.",False,False,"[0.020373698, 0.0043657473, 0.41165397, 0.016631562, 0.009640889, 0.0023078918, 0.03639866, 0.020264024]"
NickUnrelatedToPost,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Getting complex output formats like json right is beyond the abilities of 33b parameter models. Even 65b struggles hard with this.

That's the reason autogpt needs GPT3.5 at least. LLaMAs just don't cut it there.",False,False,"[0.375016, 0.011646013, 0.0069516995, 0.008880884, 0.0045707305, 0.00071525574, 0.013257195, 0.0023586133]"
involviert,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,">Even after saying only return a value etc

If that's what you're trying, then likely it can indeed be improved with a better prompt. I doubt ""returning a value"" is what ""it thinks it's doing"", therefore that definition sucks.",False,False,"[0.3208156, 0.45090583, 0.36230123, 0.013736901, 0.056346856, 0.006363631, 0.006977591, 0.42881694]"
hyajam,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Langchain provides all the underlying tools you would need for generation and parsing of JSON and similar formats using for LLMs.

Take a look at this:

https://horosin.com/extracting-pdf-and-generating-json-data-with-gpts-langchain-and-nodejs",False,False,"[0.0062396824, 0.0054258998, 0.0005340576, 0.0017481487, 0.3163961, 0.0060631246, 0.0064393696, 0.010535837]"
phocuser,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,I do it by taking the message and sending it to a new instance of an LLM and say can you please write the output into JSON format please. Can I give it an empty json document with the parts I wanted to fill out and it usually fills it out pretty well. I take regular expressions and destroy all of the output. That's not a JSON object and then I use Python to validate the object before moving forward.,False,False,"[0.35382923, 0.017811105, 0.010286893, 0.0019931793, 0.012285355, 0.026735162, 0.0056236745, 0.022628209]"
ChigGitty996,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Run many tests changing the prompt and parameters, find a seed that your results generate as expected and keep using that seed.",False,False,"[0.008492245, 0.019854378, 0.23881498, 0.0011014938, 0.009298887, 0.01402008, 0.0032373124, 0.009731233]"
RepresentativeOdd276,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Thanks! You mean this? https://github.com/microsoft/guidance/,True,False,"[0.00804228, 0.012107162, 0.0067338715, 0.0008249283, 0.0058387397, 0.006835188, 0.002210622, 0.19341932]"
SoylentMithril,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,What's the standard way people are using guidance with local models?,True,False,"[0.16286491, 0.0018683918, 0.006942749, 0.00653437, 0.006003422, 0.009869732, 0.0033616987, 0.0004553795]"
Ovidije,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Is there any difference to langchain structured output?,True,False,"[0.014839902, 0.010497936, 0.00362579, 0.013948329, 0.00704232, 0.007664877, 0.001077652, 0.15430276]"
mosquit0,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I recommend you take a look at logit_biases parameter in GPT. What I do is I increase the weight of tokens that must be present and decrease the ones that cannot be in the JSON. I do it also for all the tokens of static JSON content if I have a JSON to fill like this `{""name"": ""dynamic content""}` I would increase the weight of `{}"":name` tokens.",False,False,"[0.013131535, 0.001115799, 0.3110657, 0.008065036, 0.0038107792, 0.015386449, 0.007778878, 0.009967142]"
Sneackybae,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,">We have been working on a similar problem, with the exception we are fine-tuning our own models, so we have some control of the model output.  In our case there was better performance outputting the discrete outputs we wanted and parsing them to json than training to output JSON directly.  In this case jsonformer and MS Guidance were more error prone than a simple (coded) output.  This being said, we have used MS Guidance successfully elsewhere.  
>  
>I have not been able to find any information on how to best generate training sets for multi-class/multi-label extraction to a single JSON.  We are currently experimenting with various input and output structures.

Hey there, your project sounds really interesting and not too far off from what I'm working on. I'm focusing on real estate descriptions as well, but aiming to generate a comprehensive JSON with accurate data while also deducing some implicit information. Really, the goal is to process about a thousand lines of text every day using something like a Falcon model or any other robust open-source models.

I totally get the struggle with generating a reliable training set, especially when dealing with multi-class/multi-label extraction. I've used GPT-4 to generate my training dataset based on a carefully constructed prompt, and it's been promising. I'm sitting on about 200 rows of data right now, but I'm in a bit of a bind deciding between using something like MS Guidance coupled with fine-tuning or just going pure fine-tuning. Given that you've got some experience with MS Guidance and model outputs, do you have any pointers on what approach might yield better results?

I've been eyeballing various output structures as well—like Langchain's structured output and others. And man, it can get overwhelming. If you're up for it, maybe we could hop on a quick call to hash this out a bit more. You seem to have a good grasp on these intricacies, and I'd love to pick your brain.

For context, here's the type of output I'm aiming for, with all the possible values for each attribute:

{

""id\_original"": \[""  ""\],

""main\_type"": \[""industry"", ""house"", ""apartment"", ""kot"", ""land"", ""commercial"", ""garage"", ""office""\],

""sub\_type"": \[""industrial\_premises"", ""house"", ""apartment"", ""kot"", ""studio"", ""building\_land"", ""mixed\_use\_building\_commercial"", ""duplex"", ""lock\_up\_parking"", ""triplex"", ""mixed\_use\_building"", ""business"", ""flat\_studio"", ""warehouse"", ""covered\_parking\_space"", ""villa"", ""ground\_floor"", ""offices"", ""penthouse"", ""apartment\_block"", ""town\_house"", ""lock\_up\_garage"", ""chalet"", ""land"", ""mansion"", ""exceptional\_property"", ""land\_with\_permit""\],

""floor\_level"": \[ 0, 1, 2, 3, ...,null\],

""number\_of\_floors"": \[ 0, 1, 2, 3, ..., null\],

""room\_number"": \[null, 0, 1, 2, 3, ...\],

...

}",False,False,"[0.020901557, 0.0665887, 0.040158957, 0.31113344, 0.0036811829, 0.021585498, 0.012130005, 0.025124274]"
NemesisPrime00,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Hi what do Foo or bar mean in this case? Thanks,False,False,"[0.015796926, 0.008246276, 0.021585643, 0.2828811, 0.0044767405, 0.049928714, 0.045873325, 0.0024032593]"
RepresentativeOdd276,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Any suggestion on how it can be improved?,False,False,"[0.00086307526, 0.007145886, 0.007843964, 0.009613037, 0.007028373, 0.361374, 0.0024233595, 0.012175481]"
Hey_You_Asked,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Alexa play ""you're in the wrong subredditcito""",False,False,"[0.001115799, 0.0063367756, 0.015264924, 0.28070757, 0.0032188136, 0.030270588, 0.010085096, 0.021535112]"
harrro,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Yes,True,False,"[0.0074954215, 0.0012397766, 0.014703264, 0.16007179, 0.0077028773, 0.013508516, 0.0034777985, 0.010969753]"
harrro,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Guidance has built-in support for Transformers/local models.,True,False,"[0.21942253, 0.0023216156, 0.006657871, 0.010552917, 0.0054848767, 0.007602467, 0.0006532669, 0.006550381]"
Disastrous_Elk_6375,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"I don't know, haven't tried that. I've only tested the simple langchain stuff, found it a bit too obfuscated and convoluted for my needs. Perhaps someone with more experience can chime in.",False,False,"[0.0045692353, 0.037105743, 0.022423252, 0.016556932, 0.0013542175, 0.22944215, 0.00789674, 0.01604179]"
whereismyrapira,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,Amazing! This is good advice!,False,False,"[0.007534259, 0.007645877, 0.011435116, 0.014156717, 0.009967142, 0.3172272, 0.0028673338, 0.0012159348]"
sgt_banana1,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"JSON keys that would provide the required values. It's just one example of prompt tuning to get the desired format. For example provide JSON with the keys of summary and tldr would give you the following result: {'summary':'long summary','tldr':'too long, didn't read summary'}",False,False,"[0.014703264, 0.0035887922, 0.008690883, 0.33078048, 0.007598988, 0.008846575, 0.015016444, 0.0010681152]"
involviert,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,"Hardly, since you did not post your prompt. But for this tiny bit, I'd expect better results from something like ""respond only with blablabla"", if thats in a promt setup where you are talking to an assistant. Your general prompt format is fine? I think Aeroboros is a bit different from normal vicuna 1.1.",False,False,"[0.0068869707, 0.041915078, 0.017845914, 0.008492713, 0.019870255, 0.2599697, 0.002830336, 0.001373291]"
fpena06,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,can guidance be used with oobabooga? Thanks,True,False,"[0.015340924, 0.005512681, 0.00899713, 0.044141594, 0.023174755, 0.25169826, 0.022175416, 0.0023555756]"
harrro,Prompt Engineering: How to get open source LLMs to just return a single value or JSON output? on r/LocalLLaMA by RepresentativeOdd276,There is an unmerged PR that adds support in Oobabooga but it's easier if you just use the Guidance repo directly (they have Jupyter/Colab notebooks as well as python code samples).,True,False,"[0.008234881, 0.00092983246, 0.009200439, 0.011435116, 0.30464736, 0.0031633168, 0.007314181, 0.014771583]"
autonomousErwin,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"Hallucinations, or more correctly...Confabulations. I want to be able to get consistent results out of my LLMs and for it to be more deterministic i.e. not make stuff up. 

I've read than RAG can help with this and that might the future...",False,False,"[0.007843964, 0.01442999, 0.0010728836, 0.03109552, 0.014580919, 0.0068999166, 0.38854942, 0.001951637]"
juhsten,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"I have a character based voice bot I’m working on now and the most frustrating thing I’ve encountered in the late stages of the project is balancing the prompt with the fine tuned model. Some bot characters will have perfect personalities and identities while others, seemingly with the exact same formula, will hallucinate. Changing one word in the prompt or one training example amongst 100’s of other training examples can totally skew the bot’s personality in unpredictable ways. Feels a lot closer to an art project than a programming project.",False,False,"[0.002937317, 0.007214582, 0.27316794, 0.07054565, 0.009929225, 0.047674157, 0.02229337, 0.023995465]"
aadoop6,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"The hardest part is trusting the responses generated by the LLMs. You need to ensure that the response is factually correct with reference to provided facts. You need to ensure relevance, structure, compliance, alignment and a lot more. 

I am working on some of these issues by building a comprehensive platform for designing and managing prompts, evaluating and moderating responses. I am open to discussing this in more detail, if anyone is interested.",False,False,"[0.0035681187, 0.006220264, 0.006125955, 0.009716015, 0.0017111509, 0.25676662, 0.00049829483, 0.006724371]"
edirgl,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"I'd say Prompt Planning, and the fact that your prompts work well with a model [version.](https://version.Zero) Zero-shot Chain of Thought prompts are different for each model. If that changes all your prompt engineering work will need to be re-tuned.   
Dynamic insertion of context or LLM memory, is the second challenge.  
Confabulations/Hallucinations/Ungroundedness is third, this is inherent to any autoregressive language model though.",False,False,"[0.002885833, 0.010321004, 0.0011587143, 0.008785884, 0.017592486, 0.008116818, 0.014942379, 0.40902683]"
Tawa-online,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"The hardest part for me currently is the logistics of handling large amounts of user data. In my current project the system runs bare metal (locally) which means that caching models can be an issue, depending on the amount of models used at a time. For context this project allows you to identify trained variances in text against a large set of models. I also plan to make it possible to train custom models which again will need to be stored for use.",True,False,"[0.004125261, 0.00979021, 0.0010824203, 0.012691722, 0.014464149, 0.0077598775, 0.23844834, 0.008090926]"
DeadPukka,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. 

We recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  

Also, providing guardrails to make sure the LLM responds as expected, can be more art than science. 

We are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.",False,False,"[0.015864678, 0.36039606, 0.016587202, 0.0080798725, 0.0031818156, 0.0009822845, 0.006977591, 0.010362893]"
sergeant113,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"RAG helps, but the quality of the answer is highly dependent on the retrieval mechanism. Any poor decision during the embedding extraction pipeline can ruin the result during inference much later. It does not help the fact that LLMs do not consistently obey/disobey instructions.",True,False,"[0.02072494, 0.011046898, 0.0071588317, 0.00077724457, 0.0046002194, 0.011458136, 0.0023863618, 0.2937719]"
edirgl,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,"There is no silver bullet here. There are however lots of small things that help.  
Prompt engineering emphasizing correctness is one. If you're using a GPT endpoint tweaking temperature and top\_p to be less stochastic actually also helps. If you're using any other model making sure to tap into its temperature manifolds. Empirically I've observed that structuring your data also helps.",False,False,"[0.0072836806, 0.013849284, 0.0029413297, 0.008557882, 0.0009202957, 0.01419965, 0.25222927, 0.007314181]"
sbalnojan,What's the hardest part when building LLM applications? on r/LLMDevs by sbalnojan,That sounds like the big category of hard things is all about getting the outputs of the LLM step right.,True,False,"[0.008846575, 0.012892825, 0.007949878, 0.31513944, 0.0009202957, 0.0071717775, 0.0131943645, 0.003089321]"
gamesntech,Fine tuning LLM for Text to SQL - Self consistency on r/LocalLLaMA by weedyuh,This is one of the areas where LLMs still have ways to go. Especially if execution accuracy is very important then you can’t really use an automated process here. There are a few very good datasets that are available for training/testing though.,False,False,"[0.008256803, 0.013371054, 0.0011873245, 0.009256045, 0.007607877, 0.002830336, 0.015958898, 0.22147614]"
weedyuh,Fine tuning LLM for Text to SQL - Self consistency on r/LocalLLaMA by weedyuh,"u/gamesntech Got it, thanks. I was wondering if CoT could help.. adding a rationale or the schema links along with the Text to SQL pairs while fine tuning. This way, the precision could improve. Building CoT datasets is a cumbersome task though, there aren't any available as far as i know.",True,False,"[0.01402008, 0.0071657263, 0.00069618225, 0.3177703, 0.01005285, 0.006641001, 0.007645877, 0.0021551251]"
InTheEndEntropyWins,LLM doesnt give me a consistent result when doing a text summary on r/PromptEngineering by Fresh_Information_87,"Setting a high temperature parameter, is more likely to result in different outputs. 

You might need to rework your prompt to prevent it from using it in the output. Maybe a command to not write anything in the prompt in the output or something.",False,False,"[0.0036123516, 0.2562455, 0.0068999166, 0.00066280365, 0.0013689207, 0.01024134, 0.010245483, 0.007645877]"
blackice193,LLM doesnt give me a consistent result when doing a text summary on r/PromptEngineering by Fresh_Information_87,"On the OpenAi website if you look at API examples you will see that summaries interestingly have a Temp of 0.

A few days ago I was surprised to see the impact of Top p & nucleus settings when summarising YouTube videos. I would say start with everything on zero and experiment till you find something you like",True,False,"[0.008155655, 0.008842885, 0.002552852, 0.34327963, 0.017466826, 0.013576009, 0.0011920929, 0.008315781]"
merchantconvoy,LLM doesnt give me a consistent result when doing a text summary on r/PromptEngineering by Fresh_Information_87,Rewrite your prompt,False,False,"[0.0029043318, 0.0019073486, 0.050078966, 0.015805881, 0.008815889, 0.019676886, 0.2617214, 0.020851927]"
Necessary_Function_3,LLM doesnt give me a consistent result when doing a text summary on r/PromptEngineering by Fresh_Information_87,"not only that, it's pretty lazy and can be particularly unhelpful due it's programmed morals and ethics, eg re copyright for one example, it extends to issues getting help with issues related to standards for instance

the laziness, even when told to provide complete information and do not finish early, it stops short of listing all the items asked to locate in a text or diagram, or similar every time, has to be prompted to give more, rarely can you get everything, even when very explicit as to this being the need - don't know if this is something that OpenAI have done to reduce load or not

I asked it to provide examples of answers for my daughters english exams (it knew the book, and it knew the requirements, and my daughters teacher had not really provided examples of what a good answer might look like, though he should have, but when I asked GPT it went into moral panic about copyright, and then more moral panic about providing exam answers, I very clearly explained the issue, and how the teacher should have provided answers, and I wanted it to consider itself a teacher, and the harm that might come to a human if my daughter was so distressed about the situation she might not even attend the exam, and it cared more about doing ""the right thing"" - the right thing woudl have been to act as the teacher and provide some examples, based on another book even if this was a problem

but no, I think it has been castrated from much of it's effective use",True,False,"[0.010026119, 0.030389199, 0.0073918556, 0.002937317, 0.18364124, 0.0050317086, 0.37695956, 0.12521741]"
