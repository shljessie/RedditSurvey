Options,Selected Option
"[None, None]",
"[""I'm working on the same thing, right now, and I've had great success using one example of the format you want and using mirostat sampling. It not only improves the quality of the response (so far), but also makes the replies consistent, which is key when experimenting with prompts. Also telling it in the instructions to 'only output valid JSON' helps. I'm using chronos-hermes-13b-ggml, so not even a fancy 'coder' model or an especially large one."", ""u/gamesntech Got it, thanks. I was wondering if CoT could help.. adding a rationale or the schema links along with the Text to SQL pairs while fine tuning. This way, the precision could improve. Building CoT datasets is a cumbersome task though, there aren't any available as far as i know.""]","{'comment': ""I'm working on the same thing, right now, and I've had great success using one example of the format you want and using mirostat sampling. It not only improves the quality of the response (so far), but also makes the replies consistent, which is key when experimenting with prompts. Also telling it in the instructions to 'only output valid JSON' helps. I'm using chronos-hermes-13b-ggml, so not even a fancy 'coder' model or an especially large one."", 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"[""What's the standard way people are using guidance with local models?"", 'I found the following article interesting.\r\n\r\n[A Deep Dive Into Guidance’s Source Code](https://betterprogramming.pub/a-deep-dive-into-guidances-source-code-16681a76fb20)']","{'comment': 'I found the following article interesting.\n\n[A Deep Dive Into Guidance’s Source Code](https://betterprogramming.pub/a-deep-dive-into-guidances-source-code-16681a76fb20)', 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"['This is one of the areas where LLMs still have ways to go. Especially if execution accuracy is very important then you can’t really use an automated process here. There are a few very good datasets that are available for training/testing though.', 'Thanks! You mean this? https://github.com/microsoft/guidance/']","{'comment': 'This is one of the areas where LLMs still have ways to go. Especially if execution accuracy is very important then you can’t really use an automated process here. There are a few very good datasets that are available for training/testing though.', 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"['Is there any difference to langchain structured output?', 'While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. \r\n\r\nWe recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  \r\n\r\nAlso, providing guardrails to make sure the LLM responds as expected, can be more art than science. \r\n\r\nWe are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.']","{'comment': 'While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. \n\nWe recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  \n\nAlso, providing guardrails to make sure the LLM responds as expected, can be more art than science. \n\nWe are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.', 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"['can guidance be used with oobabooga? Thanks', ""I don't have experience with Semantic Kernel, but from a brief look at the documentation they have for it, it looks like it's supposed to be an  all-in-one sdk for for creating agents and other apps. Overall, seems like another more powerful approach at the cost of usability from Microsoft. It's hard to say what the landscape is going to look like too far into the future, so I'm not really going to attempt to make any guesses.""]","{'comment': 'can guidance be used with oobabooga? Thanks', 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'scratchr'}"
"[""The guidance transformers wrapper works with a loaded huggingface transformer model/tokenizer, so if you loaded the model with huggingface transformers and you made a custom LLM wrapper for langchain that uses huggingface transformer model/tokenizer input, you may be able to load the model once and then create/use both llm objects. Though I'm not really sure why you may want to do that."", ""You want to use the guidance library from MS. It's great for this.""]","{'comment': ""You want to use the guidance library from MS. It's great for this."", 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'scratchr'}"
"[""From what I could gather giving it a brief look, they're two completely different things that just happen to share similar markup for the prompting."", ""There's no guarantee or reason to believe that they would be more sustainable and well-researched going forward. Big companies kill projects all of the time.\r\n\r\nAlso: are Semantic Kernel and Guidance designed to be used together? My quick skim didn't provide any evidence that they are. If they are two unrelated projects then there's no reason to think of them as a unit.""]","{'comment': ""From what I could gather giving it a brief look, they're two completely different things that just happen to share similar markup for the prompting."", 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"['can guidance be used with oobabooga? Thanks', ""JSON keys that would provide the required values. It's just one example of prompt tuning to get the desired format. For example provide JSON with the keys of summary and tldr would give you the following result: {'summary':'long summary','tldr':'too long, didn't read summary'}""]","{'comment': ""JSON keys that would provide the required values. It's just one example of prompt tuning to get the desired format. For example provide JSON with the keys of summary and tldr would give you the following result: {'summary':'long summary','tldr':'too long, didn't read summary'}"", 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"[""Then why use an LLM at all at this point? Doesn't this defeat the purpose?"", 'Thanks! You mean this? https://github.com/microsoft/guidance/']","{'comment': ""Then why use an LLM at all at this point? Doesn't this defeat the purpose?"", 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
"['That sounds like the big category of hard things is all about getting the outputs of the LLM step right.', 'While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. \r\n\r\nWe recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  \r\n\r\nAlso, providing guardrails to make sure the LLM responds as expected, can be more art than science. \r\n\r\nWe are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.']","{'comment': 'While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. \n\nWe recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  \n\nAlso, providing guardrails to make sure the LLM responds as expected, can be more art than science. \n\nWe are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.', 'post_info': 'How does Microsoft Guidance work? on r/LocalLLaMA by T_hank', 'comment_author': 'TheHelpfulAssistant'}"
