Options,Selected Option
"I found the following article interesting.

[A Deep Dive Into Guidance’s Source Code](https://betterprogramming.pub/a-deep-dive-into-guidances-source-code-16681a76fb20)",4
This is one of the areas where LLMs still have ways to go. Especially if execution accuracy is very important then you can’t really use an automated process here. There are a few very good datasets that are available for training/testing though.,5
"While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. 

We recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  

Also, providing guardrails to make sure the LLM responds as expected, can be more art than science. 

We are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.",4
can guidance be used with oobabooga? Thanks,4
You want to use the guidance library from MS. It's great for this.,5
"From what I could gather giving it a brief look, they're two completely different things that just happen to share similar markup for the prompting.",4
"JSON keys that would provide the required values. It's just one example of prompt tuning to get the desired format. For example provide JSON with the keys of summary and tldr would give you the following result: {'summary':'long summary','tldr':'too long, didn't read summary'}",3
Then why use an LLM at all at this point? Doesn't this defeat the purpose?,5
"While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. 

We recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  

Also, providing guardrails to make sure the LLM responds as expected, can be more art than science. 

We are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.",4
"While building [Graphlit](https://www.graphlit.com) some of the trickiest pieces have been around prompt engineering, and getting consistency across different models. 

We recently added support for Anthropic Claude, and it behaves differently enough from OpenAI that I had to come up with a new scheme for formatting the prompt instructions and providing the source context for the response.  

Also, providing guardrails to make sure the LLM responds as expected, can be more art than science. 

We are looking at ways to automate the evals, and expand the test suite, because it gets hard to manage manually, pretty quickly.",4
You want to use the guidance library from MS. It's great for this.,5
You want to use the guidance library from MS. It's great for this.,5
